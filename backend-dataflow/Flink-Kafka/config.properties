# Table 1
table1.hostname=localhost
table1.port=3306
table1.database=dbTest13thMay
table1.table=dbTest13thMay.admin
table1.username=root
table1.password=Aa15606936638

# Table 2
table2.hostname=localhost
table2.port=3306
table2.database=dbTest13thMay
table2.table=dbTest13thMay.DataCenterAdmin
table2.username=root
table2.password=Aa15606936638

# Table 3
table3.hostname=localhost
table3.port=3306
table3.database=dbTest13thMay
table3.table=dbTest13thMay.student_course
table3.username=root
table3.password=Aa15606936638

# Table 4
table4.hostname=localhost
table4.port=3306
table4.database=dbTest13thMay
table4.table=dbTest13thMay.student_info
table4.username=root
table4.password=Aa15606936638

# Table 5
table5.hostname=localhost
table5.port=3306
table5.database=dbTest13thMay
table5.table=dbTest13thMay.student_score
table5.username=root
table5.password=Aa15606936638

# Table 6
table6.hostname=localhost
table6.port=3306
table6.database=dbTest13thMay
table6.table=dbTest13thMay.table_permissions
table6.username=root
table6.password=Aa15606936638
# Add more tables as needed...
# Table x
#tablex.hostname=xxx
#tablex.port=xxx
#tablex.database=xxx
#tablex.table=database.table
#tablex.username=xxx
#tablex.password=xxx

# doris ??

#CREATE TABLE user_info_streamload(
#UserID INT NOT NULL,
#UserName VARCHAR(30),
#Password VARCHAR(30),
#Age INT,
#Sex INT
#)
#UNIQUE KEY(UserID)
#DISTRIBUTED BY HASH(UserID) BUCKETS 1
#PROPERTIES (
#"replication_num" = "1"
#);
#
#CREATE ROUTINE LOAD user_info_routine_load_json ON user_info_streamload
#WITH MERGE
#COLUMNS(UserID,UserName,Password,Age,Sex,label),
#DELETE ON label=true
#PROPERTIES(
#"desired_concurrent_number"="1",
#"format"="json",
#"jsonpaths"="[\"$.UserID\",\"$.UserName\",\"$.Password\",\"$.Age\",\"$.Sex\",\"$.label\"]"
#)
#FROM KAFKA(
#"kafka_broker_list" = "172.21.168.87:9092",
#"kafka_topic" = "db_1151.user_info",
#"property.kafka_default_offsets" = "OFFSET_BEGINNING"
#);
#//SHOW ROUTINE LOAD FOR user_info_routine_load_json\G
#//STOP ROUTINE LOAD FOR user_info_routine_load_json;